---
title: "Big O Notation: Time and Space Complexity"
slug: big-o-notation-time-and-space-complexity

---

How fast is the code you wrote? How long will it take to run? Questions we often get asked when writing code, but there is a more elegant answer than two loops for every item. Enter Big O notation. Big O describes how fast an algorithm (the code you wrote) will run or how much memory it uses. Big O is hardware and software agnostic, meaning you can describe and compare how the algorithm works regardless of where it is running. So when you hear space and time complexity, the time complexity is how fast it will run and space complexity is how much memory it will use.

Let's focus on runtime. When using Big O this way, we are saying how well the algorithm (code) scales as we increase the number of things we put into it. Big O uses algebra to describe this abstraction using symbols like `O(1)`, `O(log n)` and `O(n)`. You might see big scary charts like this one:

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1682065771713/51073e9e-ca22-4811-b36b-9433f7b2e81c.png align="center")

Let's break down what these mean with real code examples so you can see them in action. Keep in mind the `n` stands for input.

## Constant Time

Constant time or `O(1)` means that no matter the input size, the time taken does not change. A great example of this is grabbing the first item in the array. It does not matter how big the array is; the time does not change.

```javascript
const users = ["Alexander", "Aubrey", "Linda", "Luke", "Marina", "Matt"]
const user = users[0]
```

Constant time `O(1)` is the best outcome you can aim for; however, not all code can be in constant time.

## Logarithmic Time

Logarithmic time or `O(log n)` means as the input grows, the time taken also grows but slowly. It's not a one-for-one. The algorithm will take longer to run as the input grows,, but it does not change enough to worry about once the input gets large. As the input doubles, the time taken doesn't increase by a constant amount.

Let's see this in action with some code examples. If we have a function that returns how many divisions it takes to reach zero when dividing a number by two, this is `0(log n)`. It does not matter how big the number we put in is as we divide by two each loop; the time does not increase one for one.

```javascript
function divideByTwo(num) {
  let count = 0
  
  while (num > 1) {
    num = Math.floor(num / 2);
    count += 1
  }
  
  return count
}
```

You might be thinking, Alexander that is a silly example. I have never needed to do that. You are right, but the following example is a little hard to understand if you have never done a binary search before, and I want to give everyone their best chance of understanding what Big O is.

Binary search is a great example. If you have never used binary search before (you probably have; you don't realise that is what is happening), it cuts the number of items to search in half with each loop. For example, if we have a sorted list of names, we can use binary search to find one quickly.

```javascript
const users = ["Alexander", "Aubrey", "Linda", "Luke", "Marina", "Matt"]

function searchName(names: string[], nameToFind: string) {
  let left = 0;
  let right = names.length - 1;
  
  while (left <= right) {
    // Middle index is the middle of the section of the array we are looking in
    // In the first loop it will be the name in the middle of the full array
    // In the second loop it will be the name in the middle of the first half or second half of the array depending on which side we go 
    const middleIndex = Math.floor((left + right) / 2);
    const name = names[middleIndex]
    
    if (name === nameToFind) {
      // If the name in the middle is the correct one return it
      return name
    } else if (name < nameToFind) {
      // The name were looking for comes after the name in the middle so we update left variable so in the next iteration we only look at the first half of the array
      left = middleIndex + 1;
    } else {
       // The name were looking for comes before the name in the middle so we update right variable so in the next iteration we only look at the first half of the array
      right = middleIndex - 1;
    }
  }
  
  return null; // not found
}

searchName(users, "Matt") // Returns Matt
searchName(users, "Bob") // Returns null
```

For those of you that are new to binary search, let's break down what is happening line by line.

1. We search for "Matt".
    
2. On the first loop left is 0, and the right is 5.
    
3. The middle index is 2, so we pull "Linda" from the array.
    
4. "Matt" is greater than "Linda", so the left becomes the middle index plus one.
    
5. We start the loop again. In this loop left is 3, and the right is 5. We have not cut the array in half.
    
6. The middle index is 4, and the name we pull out is "Luke"
    
7. "Matt" is greater than "Luke", so the left becomes the new middle index plus one.
    
8. We loop one more time. In this loop left is 5, and the right is 5 too. With only one item in the array, we return "Matt".
    

You can see here in each loop that we are halving the array and having to search through less. The constant halving of the number of items to check on each loop makes this logarithmic time or `O(log n)`. If you would like to learn more about binary search, check out [Vaidehi Joshi's](https://twitter.com/vaidehijoshi) blog post on [binary trees and binary search](https://medium.com/basecs/leaf-it-up-to-binary-trees-11001aaf746d).

## Linear or Constant Time

Linear time `0(n)` means that for every time the input grows the time taken grows at the same rate. The growth in time is constant. Algorithms or code that run in linear time can become slow as the input time increases. However running code that is linear is not a bad thing you just need to keep it in mind. Some classic examples you see every day.

Map through an array and create a new element.

```javascript
const users = ["Marina", "Luke", "Aubrey", "Linda", "Marina", "Matt", "Alexander"]

function createUsers(names: string[]) {
  return names.map((name, index) => {
    return {_id: index, name}
  })
}

createUsers(users)
```

Find an element in an unsorted list.

```javascript
const users = ["Marina", "Luke", "Aubrey", "Linda", "Marina", "Matt", "Alexander"]

function searchName(names: string[], nameToFind: string) {
  return names.find(name => name === nameToFind)
}

searchName(users, "Matt")
```

## Loglinear Time

Loglinear time `O(n log n)` means that for every element in the input, you're running an algorithm that operates in logarithmic time `0(log n)`. Here we see our first harsh increase as we double the input. Every time we double the input, we double the time taken plus a little bit more. Sorting algorithms are a great example of this. Let's look at two common ones Quicksort and Mergesort. Let's start with the most command that comes up a lot, Quicksort. Let's use the array of names before and sort them alphabetically.

```javascript
const users = ["Marina", "Luke", "Aubrey", "Linda", "Marina", "Matt", "Alexander"];

function sortUsersByName(users) {
  if (users.length <= 1) return users
  
  const pivotIndex = Math.floor(users.length / 2);
  const pivot = users[pivotIndex];
  console.log("Pivot: ", pivotIndex, pivot)

  const left = [];
  const right = [];

  for (let index = 0; index < users.length; index++) {
    if (index === pivotIndex) {
      continue;
    }

    if (users[index] < pivot) {
      left.push(users[index]);
    } else {
      right.push(users[index]);
    }
  }

  return sortUsersByName(left).concat(pivot, sortUsersByName(right));
}


sortUsersByName(users)
```

If you have never seen merge sort, you might be a little confused by the code there. Don't worry. Let's break it down so you can understand. Quicksort is a divide-and-conquer algorithm, meaning it breaks down the input into smaller inputs to be sorted and then adds them together. Divide and conquer is a design pattern for algorithms based on multi-branch recursion. Or, in plain English, we keep looping through the problem or input, breaking it into smaller problems or inputs until they're simple enough to be solved. We then combine the solutions. Keeping this in mind lets break down our code again.